This is the second notebook on this project. This kernel will focus on **Feature Engineering**, **Feature Selection** and **Model Building**.

First part of the Project: [Project 2 P1: EDA](https://www.kaggle.com/veb101/project-2-p1-eda)
Before starting I want to thank other kaggle users for their work on this problem. It helped me alot in understanding this problem.

This and others notebooks onn this project series relies heavily on other great kernels made on this dataset.
Naming a few:
1. [Comprehensive data exploration with Python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)
2. [A study on Regression applied to the Ames dataset](https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset)
3. [Eda and prediction of House Price](https://www.kaggle.com/siddheshpujari/eda-and-prediction-of-house-price)
3. [Stacked Regressions to predict House Prices](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)
4. [Regularized Linear Models](https://www.kaggle.com/apapiu/regularized-linear-models)
<h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#Combining-train-and-test-to-apply-transformations." data-toc-modified-id="Combining-train-and-test-to-apply-transformations.-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Combining train and test to apply transformations.</a></span></li><li><span><a href="#Feature-Engineering" data-toc-modified-id="Feature-Engineering-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Feature Engineering</a></span></li><li><span><a href="#Outlier-removal" data-toc-modified-id="Outlier-removal-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Outlier removal</a></span></li><li><span><a href="#Log-Transformation-of-target-variable" data-toc-modified-id="Log-Transformation-of-target-variable-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Log-Transformation of target variable</a></span></li><li><span><a href="#Treating-missing-values" data-toc-modified-id="Treating-missing-values-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Treating missing values</a></span><ul class="toc-item"><li><span><a href="#Numerical-features" data-toc-modified-id="Numerical-features-5.1"><span class="toc-item-num">5.1&nbsp;&nbsp;</span>Numerical features</a></span></li><li><span><a href="#Categorical-features" data-toc-modified-id="Categorical-features-5.2"><span class="toc-item-num">5.2&nbsp;&nbsp;</span>Categorical features</a></span></li></ul></li><li><span><a href="#Date-and-Time-Engineering" data-toc-modified-id="Date-and-Time-Engineering-6"><span class="toc-item-num">6&nbsp;&nbsp;</span>Date and Time Engineering</a></span></li><li><span><a href="#Variable-Transformation" data-toc-modified-id="Variable-Transformation-7"><span class="toc-item-num">7&nbsp;&nbsp;</span>Variable Transformation</a></span><ul class="toc-item"><li><span><a href="#Transforming-some-numerical-variables-that-are-really-categorical" data-toc-modified-id="Transforming-some-numerical-variables-that-are-really-categorical-7.1"><span class="toc-item-num">7.1&nbsp;&nbsp;</span>Transforming some numerical variables that are really categorical</a></span></li><li><span><a href="#Label-Encoding-some-categorical-features" data-toc-modified-id="Label-Encoding-some-categorical-features-7.2"><span class="toc-item-num">7.2&nbsp;&nbsp;</span>Label Encoding some categorical features</a></span></li><li><span><a href="#Skewed-feature-transformation" data-toc-modified-id="Skewed-feature-transformation-7.3"><span class="toc-item-num">7.3&nbsp;&nbsp;</span>Skewed feature transformation</a></span></li></ul></li><li><span><a href="#Feature-Selection" data-toc-modified-id="Feature-Selection-8"><span class="toc-item-num">8&nbsp;&nbsp;</span>Feature Selection</a></span><ul class="toc-item"><li><span><a href="#using-Lasso" data-toc-modified-id="using-Lasso-8.1"><span class="toc-item-num">8.1&nbsp;&nbsp;</span>using Lasso</a></span></li><li><span><a href="#Using-tree-based-models" data-toc-modified-id="Using-tree-based-models-8.2"><span class="toc-item-num">8.2&nbsp;&nbsp;</span>Using tree based models</a></span></li></ul></li><li><span><a href="#Model-Building" data-toc-modified-id="Model-Building-9"><span class="toc-item-num">9&nbsp;&nbsp;</span>Model Building</a></span></li><li><span><a href="#Stacking-regressor" data-toc-modified-id="Stacking-regressor-10"><span class="toc-item-num">10&nbsp;&nbsp;</span>Stacking regressor</a></span></li><li><span><a href="#Final-Run-and-Creating-Submission" data-toc-modified-id="Final-Run-and-Creating-Submission-11"><span class="toc-item-num">11&nbsp;&nbsp;</span>Final Run and Creating Submission</a></span></li></ul></div>
!pip install -qU xgboost
!pip install -qU lightgbm
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from scipy import stats
from scipy.stats import norm, skew
import os

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_validate
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.preprocessing import RobustScaler

plt.style.use("fivethirtyeight")
pd.pandas.set_option('display.max_columns', None)
sns.set_style('darkgrid')
%matplotlib inline
# train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')
# test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
print(train.shape)
print(test.shape)
test['Id'].values
# dropping ID

train.drop(['Id'], axis=1, inplace=True)

test_id = test['Id'].values # for submission
test.drop(['Id'], axis=1, inplace=True)
# Combining train and test to apply transformations.

# Feature Engineering

Feature engineering is the process of using domain knowledge of the data to transform existing features or to create new variables from existing ones, for use in machine learning.

[feature engineering](https://www.trainindata.com/post/feature-engineering-comprehensive-overview)
In this notebook we'll build on the insights we gattered from the first kernel.
We'll dive into:
1. Outlier Engineering
1. Missing Data Imputation
2. Variable Transformation
3. Date and Time Engineering
4. Categorical Encoding
5. Feature Creation (if necessary)
# Outlier removal

There are two outliers present as seen previously in the plot of `GrlivArea` and `SalePrice`
fig, ax = plt.subplots()
ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])
plt.ylabel('SalePrice', fontsize=13)
plt.xlabel('GrLivArea', fontsize=13)
plt.show()
large area but still small SalePrice
#Deleting outliers

train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)
fig, ax = plt.subplots()
ax.scatter(train['GrLivArea'], train['SalePrice'])
plt.ylabel('SalePrice', fontsize=13)
plt.xlabel('GrLivArea', fontsize=13)
plt.show()
# Log-Transformation of target variable

We preivously saw that SalePrice was heavily skewed, we would need to transform this variable
sns.distplot(train['SalePrice'] , fit=norm);

(mu, sigma) = norm.fit(train['SalePrice'])
print(f'mu = {mu:.2f} and sigma = {sigma:.2f}')

#Now plot the distribution
plt.legend([f'Normal dist. ($\mu=$ {mu:.2f} and $\sigma=$ {sigma:.2f} )'],
            loc='best')
plt.ylabel('Frequency')
plt.title('SalePrice distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(train['SalePrice'], plot=plt)
plt.show()
log transforming the feature to remove skewness
train["SalePrice"] = np.log1p(train["SalePrice"])

sns.distplot(train['SalePrice'] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(train['SalePrice'])
print(f'mu = {mu:.2f} and sigma = {sigma:.2f}')

#Now plot the distribution
plt.legend([f'Normal dist. ($\mu=$ {mu:.2f} and $\sigma=$ {sigma:.2f} )'],
            loc='best')
plt.ylabel('Frequency')
plt.title('SalePrice distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(train['SalePrice'], plot=plt)
plt.show()


---


Concatenate the train and test data in the same dataframe.
ntrain = train.shape[0]
ntest = test.shape[0]
y_train = train.SalePrice.values
all_data = pd.concat((train, test)).reset_index(drop=True)
all_data.drop(['SalePrice'], axis=1, inplace=True)
print(f"all_data size is : {all_data.shape}")
# Treating missing values
features_with_na = {feature: all_data[feature].isnull().sum() for feature in all_data.columns 
                    if all_data[feature].isnull().sum() > 0}

size = all_data.shape[0]
a = pd.DataFrame({
    'features': list(features_with_na.keys()),
    'Total': list(features_with_na.values()),
    'Missing_PCT': [np.round((features_with_na[i] / size) * 100, 3) for i in features_with_na.keys()]
}).sort_values(by='Missing_PCT', ascending=False).reset_index(drop=True)
a.style.background_gradient(cmap='Reds') 
print(f"Total number of missing values: {all_data.isna().sum().sum()}")
## Numerical features
num_with_nan = [feature for feature in features_with_na.keys() if train[feature].dtypes != 'O']
pd.DataFrame({
    'feature': num_with_nan,
    'Count': [all_data[i].isna().sum() for i in num_with_nan]
})
**Appropriate missing values for numerical variables**
* `LotFrontage`: groupby `Neighborhood` and fill the **median** value for that neighborhood.
* `MasVnrArea` - **median** or **0** as missing value suggests a no mason veneer present
*  `BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF`, `TotalBsmtSF`, `BsmtFullBath`, `BsmtHalfBath` - No basement present - **0**
* `GarageYrBlt`, `GarageCars`, `GarageArea` - no garage present - **0**

For **BsmX** variables we can drop all variables except `TotalBsmtSF`.

For **GarageX** variable we can only keep `GarageCars` as it will also give us information about the `area`.



The mean and median values of these features are different so it will be better if we fill these values with **median.**

What we can also do is create a new feature with binary values to indicate if the value was missing at that point
# LotFrontage

all_data["LotFrontage"] = all_data.groupby("Neighborhood")["LotFrontage"].apply(
    lambda x: x.fillna(x.median()))
all_data["LotFrontage"].isna().sum()
# MasVnrArea

all_data["MasVnrArea"] = all_data["MasVnrArea"].fillna(0)
all_data["MasVnrArea"].isna().sum()
# BsmtX

for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 
            'BsmtFullBath', 'BsmtHalfBath'):
    all_data[col] = all_data[col].fillna(0)

all_data[['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 
         'BsmtFullBath', 'BsmtHalfBath']].isna().sum()
# GarageX

for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):
    all_data[col] = all_data[col].fillna(0)

all_data[['GarageYrBlt', 'GarageArea', 'GarageCars']].isna().sum()
sns.heatmap(pd.DataFrame(
    {
        'BsmtFinSF1': train['BsmtFinSF1'],
        'BsmtFinSF2': train['BsmtFinSF2'], 
        'BsmtUnfSF': train['BsmtUnfSF'],
        'TotalBsmtSF': train['TotalBsmtSF'], 
        'BsmtFullBath': train['BsmtFullBath'], 
        'BsmtHalfBath': train['BsmtHalfBath'],
        'SalePrice': train['SalePrice'],
    }
).corr(), cmap='coolwarm', annot=True) 
plt.title("Bsmt numerical features - train set")
plt.show()
As suspected, `TotalBsmtSF` seems the only good choice to keep, also `BsmtFinSF1` can be kept if required. We can perform a feature selection on the features remaining at the end, to filter some more
sns.heatmap(pd.DataFrame(
    {
        'GarageYrBlt': train['GarageYrBlt'], 
        'GarageArea': train['GarageArea'],
        'GarageCars': train['GarageCars'],
        'SalePrice': train['SalePrice'],
    }).corr(), annot=True, cmap='coolwarm'
)
plt.title("Garage numerical features - train set")
plt.show()
As suspected, `GarageCars` is highly correlated with `GarageArea` and also has the highest correlation with `SalePrice`
Candidate features to be dropped:
* `BsmtFinSF2`
* `BsmtUnfSF`
* `BsmtFullBath`
* `BsmtHalfBath`
* `GarageYrBuilt`
* `GarageArea`
to_remove_ = ['BsmtFinSF2', 'BsmtUnfSF', 'BsmtFullBath', 'BsmtHalfBath', 
             'GarageYrBlt', 'GarageArea']
all_data[num_with_nan].isna().sum()
## Categorical features
cat_nan = [feature for feature in features_with_na if all_data[feature].dtypes == "O"]
pd.DataFrame({
    'feature': cat_nan,
    'Count': [all_data[i].isna().sum() for i in cat_nan]
}).sort_values(by="Count", ascending = False).reset_index(drop=True)
**Appropriate missing values for numerical variables**
* `FireplaceQu`: missing values indicates _No fireplace present_ - **None.**
* `GarageType`, `GarageFinish`, `GarageQual` & `GarageCond`: missing value indicates there's _no garage_ - **None.**
* `BsmtQual`, `BsmtCond`, `BsmtExposure`, `BsmtFinType1` & `BsmtFinType2`: missing values indicate there's _no basement_ - **None.**
* `MasVnrType`: missing value indicates there' _no masonry veneer_ - **None.**
* `MSZoning`: missing value can be filled by the most frequent value _RL_ - **mode.**
* `Functional`: from description _NA_ indicates - **Typical.**
* `Utilities`: feature can be **dropped** is heavily dominated by _AllPub_. I was wrong in determining it to be important in the EDA part.
* `Electrical`: 1 missing value, fill with most frequent value - **mode.**
* `KitchenQual`: 1 missing value, fill with most frequent value - **mode.**
* `Exterior1st` & `Exterior2nd`: 1 missing values in both, fill with most frequent value - **mode.**
* `SaleType`: 1 missing value, fill with most frequent value - **mode.**


Although, I can cannot provide the proof of it but `BsmtX` and  `GarageX` variables are mostly likely heavily correlated with each other, we can keep their numerical counterparts that convey the same information.

We can either drop these variables or keep them to see the result during feature selection.
 
In the future version, I'll update the notebook with methods to find the categorical coefficient between these variables and with the output to test my hypothesis above.
* We will drop features `PoolQC`, `MiscFeature`, `Alley` and `Fence`, `Utilities`.


to_remove_.extend(['PoolQC','MiscFeature','Alley','Fence', 'Utilities'])
plt.figure(figsize=(10, 8))
basement_variables = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 
                      'BsmtFinType1', 'BsmtFinType2']

for i, feature in enumerate(basement_variables, 1):
    plt.subplot(3, 2, i)
    sns.boxenplot(data=train, x=feature, y=train['SalePrice'])

plt.tight_layout(h_pad=1.2)
plt.show()
plt.figure(figsize=(10, 8))
garage_variables = ['GarageType', 'GarageFinish', 'GarageQual', 
                    'GarageCond']

for i, feature in enumerate(garage_variables, 1):
    plt.subplot(2, 2, i)
    sns.boxenplot(data=train, x=feature, y=train['SalePrice'])

plt.tight_layout(h_pad=1.2)
plt.show()
fill_none = ["FireplaceQu",  "GarageType", "GarageFinish", "GarageQual", "GarageCond", 
             "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "MasVnrType"]

for feature in fill_none:
    all_data[feature].fillna("None", inplace=True)

all_data[fill_none].isna().sum().sum()
fill_mode = ["MSZoning", "Electrical", "KitchenQual", "Exterior1st", 
             "Exterior2nd", "SaleType"]

for feature in fill_mode:
    all_data[feature].fillna(all_data[feature].mode()[0], inplace=True)

all_data[fill_mode].isna().sum().sum()
all_data["Functional"] = all_data["Functional"].fillna("Typ")
print(f"Number of missing values: {all_data.isna().sum().sum()}")
_____________________________
# dropping features

all_data.drop(to_remove_, axis=1, inplace=True)
all_data.shape
What we can do is keep two seperate dataframe 
1. Consisting of categorical `GarageX` and `BsmtX` variables.
2. One Without.
all_data_cat = all_data.copy()
all_data_cat.shape
all_data_free = all_data.copy()
remove_cat_garage_bsmt = ["GarageType", "GarageFinish", "GarageQual", "GarageCond", 
             "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2"]
all_data_free.drop(remove_cat_garage_bsmt, axis=1, inplace=True)
all_data_free.shape
# Date and Time Engineering


What we could do is express the 3 year variables in term of time intervals by subracting their value from `YrSold` as done during EDA.
We can then replace the 3 variable with the one that shows the highest correlation with the `SalePrice`.
# Variable Transformation
## Transforming some numerical variables that are really categorical
# MSSubClass=The building class
all_data_cat['MSSubClass'] = all_data_cat['MSSubClass'].apply(str)


# Changing OverallCond into a categorical variable
all_data_cat['OverallCond'] = all_data_cat['OverallCond'].astype(str)


# Year and month sold are transformed into categorical features.
all_data_cat['YrSold'] = all_data_cat['YrSold'].astype(str)
all_data_cat['MoSold'] = all_data_cat['MoSold'].astype(str)

# same transformation to other dataframe
# MSSubClass=The building class
all_data_free['MSSubClass'] = all_data_free['MSSubClass'].apply(str)


# Changing OverallCond into a categorical variable
all_data_free['OverallCond'] = all_data_free['OverallCond'].astype(str)


# Year and month sold are transformed into categorical features.
all_data_free['YrSold'] = all_data_free['YrSold'].astype(str)
all_data_free['MoSold'] = all_data_free['MoSold'].astype(str)
## Label Encoding some categorical features
from sklearn.preprocessing import LabelEncoder


cols = ['BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', 
        'CentralAir', 'ExterCond', 'ExterQual', 'FireplaceQu', 'Functional', 
        'GarageCond', 'GarageFinish', 'GarageQual', 'HeatingQC', 'KitchenQual', 
        'LandSlope', 'LotShape', 'MSSubClass', 'MoSold', 'OverallCond', 'PavedDrive', 
         'Street', 'YrSold']

for feature in cols:
    encoder = LabelEncoder()
    encoder.fit(all_data_cat[feature].values)
    all_data_cat[feature] = encoder.transform(all_data_cat[feature].values)


print(f'Shape all_data_cat: {all_data_cat.shape}')

# Same for all_data_free

cols = ['CentralAir', 'ExterCond', 'ExterQual', 'FireplaceQu', 'Functional', 
        'HeatingQC', 'KitchenQual', 'LandSlope', 'LotShape', 'MSSubClass', 
        'MoSold', 'OverallCond', 'PavedDrive', 'Street', 'YrSold']

for feature in cols:
    encoder = LabelEncoder()
    encoder.fit(all_data_free[feature].values)
    all_data_free[feature] = encoder.transform(all_data_free[feature].values)



print(f'Shape all_data_free: {all_data_free.shape}')
According to various top kernels total square footage might be an importatnt feature. Giving it a moments thought we can see why.
all_data_cat['TotalSF'] = all_data_cat['TotalBsmtSF'] + all_data_cat['1stFlrSF'] + all_data_cat['2ndFlrSF']

all_data_free['TotalSF'] = all_data_free['TotalBsmtSF'] + all_data_free['1stFlrSF'] + all_data_free['2ndFlrSF']
## Skewed feature transformation

During EDA we saw many continuous numeric features that were skewed. In this section we'll apply a Box-Cox transformation on them.
# all_data_cat

numeric_feature_cat = all_data_cat.select_dtypes("number").columns

# Check the skew of all numerical features
skewed_feats_cat = all_data_cat[numeric_feature_cat].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)
print("\nSkew in numerical features: \n")
skewness_cat = pd.DataFrame({'Skew' :skewed_feats_cat})
skewness_cat.head(10)
# all_data_free

numeric_feature_free = all_data_free.select_dtypes("number").columns

# Check the skew of all numerical features
skewed_feats_free = all_data_free[numeric_feature_free].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)
print("\nSkew in numerical features: \n")
skewness_free = pd.DataFrame({'Skew' :skewed_feats_free})
skewness_free.head(10)

**Box Cox Transformation of (highly) skewed features**

We use the scipy function **boxcox1p** which computes the Box-Cox transformation of  $1+x$ .

Note that setting  $Î»=0$  is equivalent to $log1p$ used above for the target variable.

See [this](http://onlinestatbook.com/2/transformations/box-cox.html) page for more details on Box Cox Transformation as well as [the scipy function's page](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.boxcox1p.html)


# for all_data_cat

from scipy.special import boxcox1p

skewness_cat = skewness_cat[abs(skewness_cat) > 0.75]
print("There are {} skewed numerical features to Box Cox transform".format(skewness_cat.shape[0]))

skewed_features_cat = skewness_cat.index
lam = 0.15
for feat in skewed_features_cat:
    all_data_cat[feat] = boxcox1p(all_data_cat[feat], lam)
# all_data_free

skewness_free = skewness_free[abs(skewness_free) > 0.75]
print("There are {} skewed numerical features to Box Cox transform".format(skewness_free.shape[0]))

skewed_features_free = skewness_free.index
lam = 0.15
for feat in skewed_features_free:
    #all_data[feat] += 1
    all_data_free[feat] = boxcox1p(all_data_free[feat], lam)
 
print(f"all_data_cat.shape: {all_data_cat.shape}")
print(f"all_data_free.shape: {all_data_free.shape}")
all_data_free.head()
# apply min max scaler first
all_data_cat = pd.get_dummies(all_data_cat)
print(all_data_cat.shape)
all_data_free = pd.get_dummies(all_data_free)
print(all_data_free.shape)
all_data_free[numeric_feature_free].head()
train_cat = all_data_cat[:ntrain]
test_cat = all_data_cat[ntrain:]
train_free = all_data_free[:ntrain]
test_free = all_data_free[ntrain:]
train_cat.shape, test_cat.shape
train_free.shape, test_free.shape
# Using MinMaxScaler
X_cat = train_cat.copy()
cols_cat = X_cat.columns

scaler_cat = MinMaxScaler()

# fitting MinaMaxScaler to training data
scaler_cat.fit(X_cat)

# transforming training and test data
X_cat = scaler_cat.transform(X_cat)
test_cat = scaler_cat.transform(test_cat)


X_cat = pd.DataFrame(X_cat, columns=[cols_cat])
test_cat = pd.DataFrame(test_cat, columns=[cols_cat])
X_cat.shape, test_cat.shape
X_free = train_free.copy()
cols_free = X_free.columns

scaler_free = MinMaxScaler()

# fitting MinaMaxScaler to training data
scaler_free.fit(X_free)

# transforming training and test data
X_free = scaler_free.transform(X_free)
test_free = scaler_free.transform(test_free)


X_free = pd.DataFrame(X_free, columns=[cols_free])
test_free = pd.DataFrame(test_free, columns=[cols_free])
X_free.shape, test_free.shape
y = y_train
# Feature Selection
## using Lasso
# Using SelectFromModel with lasso for selecting best features
# for X_cat
lasso = Pipeline([
    ("scaler", RobustScaler()), 
    ("ls", Lasso(alpha =0.0005, random_state=1))
])

feature_sel_model = SelectFromModel(lasso).fit(X_cat, y)

coefficiets = feature_sel_model.estimator_['ls'].coef_
X_cat_cols = []

for i, j in enumerate(coefficiets):
    if j != 0:
        X_cat_cols.append(X_cat.columns[i])


# selected_feat_cat = X_cat.columns[(feature_sel_model.get_support())]
print(len(X_cat_cols))

X_cat_lasso = X_cat[X_cat_cols].reset_index(drop=True)
test_cat_lasso = test_cat[X_cat_cols].reset_index(drop=True)

print(X_cat_lasso.shape, test_cat_lasso.shape)
We came down from 206 features to 83
# for X_free
lasso = Pipeline([
    ("scaler", RobustScaler()), 
    ("ls", Lasso(alpha =0.0005, random_state=1))
])

feature_sel_model = SelectFromModel(lasso).fit(X_free, y)

coefficiets = feature_sel_model.estimator_['ls'].coef_
X_free_cols = []

for i, j in enumerate(coefficiets):
    if j != 0:
        X_free_cols.append(X_free.columns[i])


print(len(X_free_cols))

X_free_lasso = X_free[X_free_cols].reset_index(drop=True)
test_free_lasso = test_free[X_free_cols].reset_index(drop=True)

print(X_free_lasso.shape, test_free_lasso.shape)
We came down from 191 features to 77
## Using tree based models


tree_models = {
    "Light_GBM": LGBMRegressor(objective='regression',num_leaves=5,
                              learning_rate=0.05, n_estimators=1200,
                              max_bin = 55, bagging_fraction = 0.8,
                              bagging_freq = 5, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf=6, min_sum_hessian_in_leaf = 11),
    
    "XGBoost": XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, 
                 max_depth=5, min_child_weight=1.7817, n_estimators=2200,
                 reg_alpha=0.4640, reg_lambda=0.8571, subsample=0.5213,
                 nthread = -1, objective="reg:squarederror", random_state=42), 
    
    "Gradient_boosting": GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, 
                                                   max_depth=6, min_samples_split=17, 
                                                   max_features='sqrt', min_samples_leaf=13, 
                                                   loss='huber', random_state=42),
               
    "Extra_trees": ExtraTreesRegressor(n_estimators=2000, max_depth=9, min_samples_split= 13, 
                        max_leaf_nodes=11, min_weight_fraction_leaf=0.39, max_features='sqrt', 
                        n_jobs=-1, random_state=42),
}
Demo: feature selection using **extra trees**
# for X_cat

print("For X_cat - Extra Trees")
selection_extra_cat = SelectFromModel(tree_models["Extra_trees"]).fit(X_cat, y_train)

selected_feat_extra_cat = X_cat.columns[(selection_extra_cat.get_support())]
print("Number of selected features:", len(selected_feat_extra_cat))

X_cat_extra = X_cat[selected_feat_extra_cat].reset_index(drop=True)
test_cat_extra = test_cat[selected_feat_extra_cat].reset_index(drop=True)
print("Transformed shape: ", X_cat_extra.shape, test_cat_extra.shape)

# uncomment next line to print selected features
# print(X_cat_extra.columns)
# for X_free

print("\nFor X_free - Extra Trees")

selection_extra_free = SelectFromModel(tree_models["Extra_trees"]).fit(X_free, y_train)

selected_feat_extra_free = X_free.columns[(selection_extra_free.get_support())]
print("Number of selected features:", len(selected_feat_extra_free))

X_free_extra = X_free[selected_feat_extra_free].reset_index(drop=True)
test_free_extra = test_free[selected_feat_extra_free].reset_index(drop=True)
print("Transformed shape: ", X_free_extra.shape, test_free_extra.shape)

# uncomment next line to print selected features
# print(X_cat_extra.columns)
# Model Building

We'll run a 7-Fold cross validation of **X_cat** and **X_free** to determine which if _BsmtX_ and _GarageX_ variables removed have any effect on the prediction. 

Also we'll 

We'll train our data on 4 Boosting trees algorithms:
1. XGBoost
2. Gradient Boosting trees
3. Light GBM
4. Extra trees

* We can built a Pipeline of _RobustScaler_ , _tree_based_ selection and _Final model_ to see how they work.
* If time permits I'll add this in future versions.
* For now we'll directly use a tree based models to fit and predict with cross-validation.

def fit_model(model_name, model, X, y, n_folds=7, show=10):
    X = X.copy()
    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)
    output = cross_validate(model, X.values, y, scoring="neg_mean_squared_error", cv=kf, return_estimator=True)
    
    feat_scores = pd.DataFrame(index=X.columns)

    for idx,estimator in enumerate(output['estimator']):
        temp = pd.Series(estimator.feature_importances_, index=X.columns)
        feat_scores[str(idx)] = temp

    feat_scores.reset_index(inplace=True)

    feat_scores['Importance'] = feat_scores.mean(axis=1)

    feat_scores = feat_scores.sort_values(by="Importance", ascending=False)[['level_0', "Importance"]].head(show)

    plt.figure(figsize=(15, 10))
    plt.gca()
    sns.barplot(x=feat_scores['Importance'], y=feat_scores['level_0'])
    plt.ylabel("Features")
    plt.title(f"Feature Importance: {model_name}")

    print(f"{model_name} score => RMSE: {np.round(np.mean(np.sqrt(-output['test_score'])), 4)}, std: {np.round(np.std(-output['test_score']), 4)}")
    
    plt.show()
    print("-------------------------------------------------------------------------------------------------")
Running the next cell will for each **X_set** created perform a cross validation run using all tree based model, plot best 30 features that are most important.
print("Running boosting tree on X_cat_lasso, X_cat_free, X_cat, X_free")

training_data = {
    "X_cat_lasso": X_cat_lasso, "X_free_lasso": X_free_lasso,
    "X_cat": X_cat, "X_free": X_free, 
}


for X_name, X in training_data.items():
    print(f"X => {X_name}, shape: {X.shape}\n")
    for model_name, model in tree_models.items():
        fit_model(model_name, model, X, y, n_folds=7, show=35)
    print()
# Stacking regressor
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.svm import LinearSVR
from sklearn.metrics import mean_squared_error
from sklearn.base import clone

def rmsle(y, y_pred):
    return np.sqrt(mean_squared_error(y, y_pred))

level0 = list()
# level0.extend(('LGB', tree_models["Light_GBM"]))
level0.append(('xgb', tree_models["XGBoost"]))
level0.append(('GBT', tree_models["Gradient_boosting"]))
level0.append(('xtra', tree_models["Extra_trees"]))

training_data = {"X_cat_lasso": X_cat_lasso, "X_free_lasso": X_free_lasso,
                 "X_cat": X_cat, "X_free": X_free, 
                 }


level1_models = {
    "Linear": LinearRegression(), 
#     "Linear_SVR":  LinearSVR(epsilon=1.5),
#     "MLP": MLPRegressor(hidden_layer_sizes=(2), activation='logistic', solver='sgd', 
#                         max_iter=1000, learning_rate='adaptive', random_state=42)
}
**Private Run**


1. X => X_cat_lasso, shape: (1458, 83)
    * Model: Linear, R2: 0.9851682263983834, RMSE: 0.04847376103391416
    * Model: Linear_SVR, R2: -6.633282458849109, RMSE: 0.7302971301321612
    * Model: MLP, R2: -2983940.9730241755, RMSE: 0.39935765079237545
    

2. X => X_free_lasso, shape: (1458, 77)
    * Model: Linear, R2: 0.981016025855836, RMSE: 0.05452379484010679
    * Model: Linear_SVR, R2: -6.461431642367199, RMSE: 0.7167880791589558
    * Model: MLP, R2: -2982300.610854054, RMSE: 0.39935750649014945


3. X => X_cat, shape: (1458, 206)
    * Model: Linear, R2: 0.9818263620627218, RMSE: 0.053600087870622565
    * Model: Linear_SVR, R2: -6.812972415367375, RMSE: 0.7336647698772716
    * Model: MLP, R2: -2964776.4375035004, RMSE: 0.3993568326589984


4. X => X_free, shape: (1458, 191)
    * Model: Linear, R2: 0.9825960824103169, RMSE: 0.05235665694025279
    * Model: Linear_SVR, R2: -6.6057353381424395, RMSE: 0.7214971271278083
    * Model: MLP, R2: -2920872.063956515, RMSE: 0.3993558505344274
# To see outputs, uncomment the next lines 

# for X_name, X in training_data.items():
#     print("==========================================")
#     X = X.copy()
#     print(f"X => {X_name}, shape: {X.shape}\n")

#     for name, level1 in level1_models.items():
#         level1_ = clone(level1)
#         model_ = StackingRegressor(estimators=level0, final_estimator=level1_, cv=7, n_jobs=-1)
#         model_.fit(X, y)
#         y_preds = model_.predict(X)
#         print(f"Model: {name}, R2: {r2_score(y_preds, y)}, RMSE: {rmsle(y_preds, y)}")
# Final Run and Creating Submission
model_ = StackingRegressor(estimators=level0, final_estimator=level1_models['Linear'], cv=7, n_jobs=-1)
model_.fit(X_cat_lasso, y)
y_preds = model_.predict(X_cat_lasso)
print(f"Model: Linear, R2: {r2_score(y_preds, y)}, RMSE: {rmsle(y_preds, y)}")
y_preds = model_.predict(test_cat_lasso)
# print(y_preds.shape, test_id.shape)
my_submission = pd.DataFrame({'Id': test_id, 'SalePrice': y_preds})
# you could use any filename. We choose submission here
my_submission.to_csv('submission.csv', index=False)
